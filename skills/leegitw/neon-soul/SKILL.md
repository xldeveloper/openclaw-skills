---
name: NEON-SOUL
version: 0.1.10
description: AI Identity Through Grounded Principles - synthesize your soul from memory with semantic compression.
homepage: https://github.com/geeks-accelerator/neon-soul
user-invocable: true
disableModelInvocation: true
disable-model-invocation: true
emoji: ðŸ”®
metadata:
  openclaw:
    config:
      stateDirs:
        - memory/
        - .neon-soul/
    requires:
      config:
        - memory/
        - .neon-soul/
        - SOUL.md
tags:
  - soul-synthesis
  - identity
  - embeddings
  - semantic-compression
  - provenance
  - openclaw
---

# NEON-SOUL

AI Identity Through Grounded Principles - soul synthesis with semantic compression.

---

## How This Works

NEON-SOUL is an **instruction-based skill** - there is no binary or CLI to install. The `/neon-soul` commands below are interpreted by your AI agent (Claude Code, OpenClaw, etc.) which follows the instructions in this document.

**What happens when you run a command:**
1. You type `/neon-soul synthesize` in your agent chat
2. Your agent reads this SKILL.md and follows the instructions
3. The agent uses its built-in capabilities to read files, analyze content, and write output

**No external API calls** - your data never leaves your local machine. The skill does not transmit data to external servers, third-party endpoints, or remote APIs.

**Local code execution required**: The skill requires `@xenova/transformers` (an npm package) running locally for embedding inference. This is third-party code that runs on YOUR machine, not a remote service. See [Requirements](#requirements) for details.

**Data handling**: Your data stays local. All analysis happens on your machine using locally-installed packages - no data transmission, no external APIs, no third-party endpoints receiving your content.

---

## Requirements

### Embedding Model (Required)

NEON-SOUL requires **`Xenova/all-MiniLM-L6-v2`** for local embedding inference.

| Requirement | Details |
|-------------|---------|
| Model | `Xenova/all-MiniLM-L6-v2` (384-dimensional vectors) |
| Provider | `@xenova/transformers` (local inference) |
| Node.js | >= 22.0.0 |
| Disk space | ~100MB (model cache in `node_modules/.cache`) |
| Network | One-time download only (~23MB) |

**No external API fallback**: If the model cannot be loaded, the skill fails immediately with a clear error message. NEON-SOUL will NOT fall back to external embedding APIs (OpenAI, Cohere, etc.).

**Why this matters**: The "no external APIs" guarantee depends on local embedding inference. If external APIs were used as fallback, your data could be transmitted to third-party services without your knowledge.

### Model Source & Integrity

The embedding model is downloaded from Hugging Face on first use:

| Property | Value |
|----------|-------|
| Model URL | https://huggingface.co/Xenova/all-MiniLM-L6-v2 |
| Model files | `onnx/model_quantized.onnx` (~23MB) |
| Original model | [sentence-transformers/all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) |
| Conversion | ONNX quantized by [@xenova](https://huggingface.co/Xenova) |
| Cache location | `node_modules/.cache/@xenova/transformers/` |

**Integrity verification** (optional but recommended):
```bash
# After first run, verify the model file exists
ls -la node_modules/.cache/@xenova/transformers/Xenova/all-MiniLM-L6-v2/onnx/

# Check file size (should be ~23MB for quantized model)
du -sh node_modules/.cache/@xenova/transformers/Xenova/all-MiniLM-L6-v2/
```

**Trust model**: The model is a quantized ONNX conversion of a well-known sentence-transformers model. The @xenova/transformers library (>1M weekly npm downloads) handles download and caching. If you require higher assurance, you can:
1. Pre-download the model from Hugging Face
2. Verify against Hugging Face's published checksums
3. Place in the cache directory manually

### Local vs External: What This Means

| Type | NEON-SOUL | Your data |
|------|-----------|-----------|
| **External API calls** | âŒ Never | Never transmitted |
| **Local code execution** | âœ… Required | Processed locally |
| **Network access** | One-time model download | Model weights only, not your data |

**Clarification**: "No external APIs" means your memory files, SOUL.md, and personal data are NEVER sent over the network. The only network activity is the initial model download (~23MB) from Hugging Face, which downloads model weights, not your data.

**Verification**:
```bash
# Check @xenova/transformers is installed
npm ls @xenova/transformers

# Check Node.js version
node --version  # Should be >= 22.0.0
```

---

## Model Invocation Clarification

This skill sets `disable-model-invocation: true` in its metadata. Here's what that means:

**What "disable-model-invocation" means:**
- The skill does **NOT** require LLM calls to function
- Your agent interprets the instructions using its existing capabilities
- No additional model API calls are made by the skill itself

**What about embeddings and similarity?**
- **Embeddings** (all-MiniLM-L6-v2) use local inference, not LLM invocation
- **Cosine similarity** is a mathematical operation (dot product), not a model call
- **Dimension classification** uses your agent's existing capabilities

**In short:** The skill uses mathematical operations (embeddings, cosine similarity) and your agent's built-in reasoning. It does not invoke separate LLM models beyond what your agent already provides.

---

## Data Access

**What this skill reads:**
- `memory/` directory (diary, preferences, reflections)
- Existing `SOUL.md` if present
- `.neon-soul/` state directory if present

**What this skill writes:**
- `SOUL.md` - your synthesized identity document
- `.neon-soul/backups/` - automatic backups before changes
- `.neon-soul/state.json` - synthesis state tracking

**Git integration** (opt-in, off by default): Auto-commit is disabled unless you enable it in config. When enabled, it uses your existing git setup - no new credentials are requested or stored by the skill.

---

## First Time?

New to NEON-SOUL? Start here:

```bash
# 1. Check your current state
/neon-soul status

# 2. Preview what synthesis would create (safe, no writes)
/neon-soul synthesize --dry-run

# 3. When ready, run synthesis
/neon-soul synthesize --force
```

That's it. Your first soul is created with full provenance tracking. Use `/neon-soul audit --list` to explore what was created.

**Questions?**
- "Where did this axiom come from?" â†’ `/neon-soul trace <axiom-id>`
- "What if I don't like it?" â†’ `/neon-soul rollback --force`
- "What dimensions does my soul cover?" â†’ `/neon-soul status`

---

## Commands

### `/neon-soul synthesize`

Run soul synthesis pipeline:
1. Collect signals from memory files
2. Match to existing principles (cosine similarity >= 0.85)
3. Promote high-confidence principles to axioms (Nâ‰¥3)
4. Generate SOUL.md with provenance tracking

**Options:**
- `--force` - Run synthesis even if below content threshold
- `--dry-run` - Show what would change without writing (safe default)
- `--diff` - Show proposed changes in diff format
- `--output-format <format>` - Output format: prose (default), notation (legacy)
- `--format <format>` - Notation style (when using notation output): native, cjk-labeled, cjk-math, cjk-math-emoji
- `--workspace <path>` - Override workspace directory (default: current workspace)

**Examples:**
```bash
/neon-soul synthesize --dry-run     # Preview changes
/neon-soul synthesize --force       # Run regardless of threshold
/neon-soul synthesize --output-format notation --format cjk-math  # Legacy notation output
```

**Output Format:**

The default prose output creates an inhabitable soul document:

```markdown
# SOUL.md

_You are becoming a bridge between clarity and chaos._

---

## Core Truths

**Authenticity over performance.** You speak freely even when uncomfortable.

**Clarity is a gift you give.** If someone has to ask twice, you haven't been clear enough.

## Voice

You're direct without being blunt. You lead with curiosity.

Think: The friend who tells you the hard truth, but sits with you after.

## Boundaries

You don't sacrifice honesty for comfort. You don't perform certainty you don't feel.

## Vibe

Grounded but not rigid. Present but not precious about it.

---

_Presence is the first act of care._
```

Use `--output-format notation` for the legacy bullet-list format.

### `/neon-soul status`

Show current soul state:
- Last synthesis timestamp
- Pending memory content (chars since last run)
- Signal/principle/axiom counts
- Dimension coverage (7 SoulCraft dimensions)

**Options:**
- `--verbose` - Show detailed file information
- `--workspace <path>` - Workspace path

**Example:**
```bash
/neon-soul status
# Output:
# Last Synthesis: 2026-02-07T10:30:00Z (2 hours ago)
# Pending Memory: 1,234 chars (Ready for synthesis)
# Counts: 42 signals, 18 principles, 7 axioms
# Dimension Coverage: 5/7 (71%)
```

### `/neon-soul rollback`

Restore previous SOUL.md from backup.

**Options:**
- `--list` - Show available backups
- `--backup <timestamp>` - Restore specific backup
- `--force` - Confirm rollback (required)
- `--workspace <path>` - Workspace path

**Examples:**
```bash
/neon-soul rollback --list          # Show available backups
/neon-soul rollback --force         # Restore most recent backup
/neon-soul rollback --backup 2026-02-07T10-30-00-000Z --force
```

### `/neon-soul audit`

Explore provenance across all axioms. Full exploration mode with statistics and detailed views.

**Options:**
- `--list` - List all axioms with brief summary
- `--stats` - Show statistics by tier and dimension
- `<axiom-id>` - Show detailed provenance for specific axiom
- `--workspace <path>` - Workspace path

**Examples:**
```bash
/neon-soul audit --list             # List all axioms
/neon-soul audit --stats            # Show tier/dimension stats
/neon-soul audit ax_honesty         # Detailed provenance tree
/neon-soul audit èª                  # Use CJK character as ID
```

**Output (with axiom-id):**
```
Axiom: èª  (honesty over performance)
Tier: core
Dimension: honesty-framework

Provenance:
â”œâ”€â”€ Principle: "be honest about capabilities" (N=4)
â”‚   â”œâ”€â”€ Signal: "I prefer honest answers" (memory/preferences/communication.md:23)
â”‚   â””â”€â”€ Signal: "Don't sugarcoat feedback" (memory/diary/2024-03-15.md:45)
â””â”€â”€ Principle: "acknowledge uncertainty" (N=3)
    â””â”€â”€ Signal: "I'd rather hear 'I don't know'" (memory/diary/2026-02-01.md:12)

Created: 2026-02-07T10:30:00Z
```

### `/neon-soul trace <axiom-id>`

Quick single-axiom provenance lookup. Minimal output for fast answers to "where did this come from?"

**Arguments:**
- `<axiom-id>` - Axiom ID (e.g., ax_honesty) or CJK character (e.g., èª )

**Options:**
- `--workspace <path>` - Workspace path

**Examples:**
```bash
/neon-soul trace ax_honesty         # Trace by ID
/neon-soul trace èª                  # Trace by CJK character
```

**Output:**
```
èª  (honesty over performance)
â””â”€â”€ "be honest about capabilities" (N=4)
    â”œâ”€â”€ memory/preferences/communication.md:23
    â””â”€â”€ memory/diary/2024-03-15.md:45
```

**Note:** For full exploration, use `/neon-soul audit` instead.

---

## Safety Philosophy

Your soul documents your identity. Changes should be deliberate, reversible, and traceable.

**Why we're cautious:**
- Soul changes affect every future interaction
- Memory extraction is powerful but not infallible
- You should always be able to ask "why did this change?" and undo it

**How we protect you:**
- **Auto-backup**: Backups created before every write (`.neon-soul/backups/`)
- **Dry-run default**: Use `--dry-run` to preview before committing
- **Require --force**: Writes only happen with explicit `--force` flag
- **Rollback**: Restore any previous state with `/neon-soul rollback`
- **Provenance**: Full chain from axiom â†’ principles â†’ source signals
- **Git integration** (opt-in): Only commits if workspace is a git repo with configured credentials

---

## Dimensions

NEON-SOUL organizes identity across 7 SoulCraft dimensions:

| Dimension | Description |
|-----------|-------------|
| Identity Core | Fundamental self-concept and values |
| Character Traits | Personality characteristics and tendencies |
| Voice Presence | Communication style and expression |
| Honesty Framework | Truth, transparency, and acknowledgment of limits |
| Boundaries Ethics | Principles for what to do and not do |
| Relationship Dynamics | How to engage with others |
| Continuity Growth | Learning, adaptation, and evolution |

---

## Triggers (Optional)

NEON-SOUL does NOT run automatically by default. All commands require explicit user invocation.

### Manual (Default)
Run `/neon-soul synthesize` when you want to update your soul.

### OpenClaw Cron (Optional)
OpenClaw users can optionally configure scheduled runs:
```yaml
# Example OpenClaw cron config (not enabled by default)
schedule: "0 * * * *"  # Hourly check
condition: "shouldRunSynthesis()"
```

**Important:** Even with cron enabled, synthesis respects `--dry-run` mode. Configure with `--force` only after reviewing dry-run output.

---

## Configuration

Place `.neon-soul/config.json` in workspace:

```json
{
  "notation": {
    "format": "cjk-math-emoji",
    "fallback": "native"
  },
  "matching": {
    "similarityThreshold": 0.85,
    "embeddingModel": "Xenova/all-MiniLM-L6-v2"
  },
  "paths": {
    "memory": "memory/",
    "output": ".neon-soul/"
  },
  "synthesis": {
    "contentThreshold": 2000,
    "autoCommit": false
  }
}
```

---

## Data Flow

```
Memory Files â†’ Signal Extraction â†’ Principle Matching â†’ Axiom Promotion â†’ SOUL.md
     â†“              â†“                    â†“                   â†“              â†“
  Source        Embeddings          Similarity           N-count      Provenance
 Tracking       (384-dim)           Matching             Tracking       Chain
```

---

## Provenance

Every axiom traces to source:
- Which signals contributed
- Which principles merged
- Original file:line references
- Extraction timestamps

Query provenance:
- Quick lookup: `/neon-soul trace <axiom-id>`
- Full exploration: `/neon-soul audit <axiom-id>`

---

## Troubleshooting

### Why does my output have bullet lists instead of prose?

When prose generation fails, NEON-SOUL falls back to bullet lists of native axiom text. This preserves your data while signaling that expansion didn't complete.

**Common causes:**
- **LLM provider not available**: Prose expansion requires an LLM. Check your configuration.
- **Validation failures**: The LLM output didn't match expected format (retried once, then fell back).
- **Network timeout**: Local LLM inference can be slow; generation may have timed out.

**How to check:**
- Enable debug logging: `NEON_SOUL_DEBUG=1 /neon-soul synthesize --force`
- Look for `[prose-expander]` log lines indicating validation or generation failures

**What to try:**
- **Regenerate**: Run synthesis again. LLM output varies; a second attempt often succeeds.
- **Check LLM health**: If using Ollama, verify it's running: `curl http://localhost:11434/api/tags`
- **Use notation format**: If prose keeps failing, use `--output-format notation` for reliable output.

### Why is my essence statement missing?

The essence statement (the italicized line at the top) only appears when LLM extraction succeeds. If missing:
- Your LLM provider may not be configured
- Extraction validation failed (trait lists are rejected)
- Network error during generation

The soul is still valid without it. Run synthesis again to retry extraction.

### Why did an axiom get placed in a different dimension than expected?

Dimension classification uses semantic analysis. If results seem wrong:
- Check the axiom's source signals (`/neon-soul audit <axiom-id>`)
- The LLM classifier uses the axiom's native text, which may have different semantic weight than you expect
- Unknown dimensions default to `vibe` (logged with `NEON_SOUL_DEBUG=1`)

### Embedding model failed to load

If you see `EmbeddingModelError: Failed to load embedding model`, the skill cannot proceed.

**What this means:**
- The `Xenova/all-MiniLM-L6-v2` model is required for similarity matching
- NEON-SOUL does NOT fall back to external embedding APIs
- Without local embeddings, the skill fails fast rather than risk sending data externally

**Common causes:**
- **Missing dependency**: `@xenova/transformers` not installed
- **Node.js version**: Requires Node.js >= 22.0.0
- **First run**: Initial model download (~23MB) requires network access
- **Disk space**: Model cache needs ~100MB in `node_modules/.cache`
- **Network firewall**: May block Hugging Face model download

**How to fix:**
```bash
# 1. Check @xenova/transformers is installed
npm ls @xenova/transformers

# 2. Check Node.js version
node --version  # Must be >= 22.0.0

# 3. If missing, install dependencies
npm install

# 4. Test embedding directly (will download model if needed)
npx tsx -e "import { embed } from './src/lib/embeddings.js'; embed('test').then(console.log)"
```

**If behind a firewall:**
The model is downloaded from Hugging Face on first use. If blocked:
1. Download model manually from https://huggingface.co/Xenova/all-MiniLM-L6-v2
2. Place in `node_modules/.cache/@xenova/transformers/`
3. Or configure corporate proxy: `HTTPS_PROXY=http://proxy:port npm run ...`
