# SKILL.md - Web Scraper Skill

# Web Scraper - é€šç”¨ç½‘é¡µçˆ¬è™«å’Œæ•°æ®æŠ“å–å·¥å…·

## ç®€ä»‹
Web Scraper æ˜¯ä¸€ä¸ªåŠŸèƒ½å¼ºå¤§çš„ç½‘é¡µçˆ¬è™«å’Œæ•°æ®æŠ“å–å·¥å…·ï¼Œä¸“ä¸º AI Agent è®¾è®¡ã€‚å®ƒèƒ½å¤ŸæŠ“å–ç½‘é¡µå†…å®¹ã€çˆ¬å–æ•´ä¸ªç½‘ç«™ã€æœç´¢ç½‘é¡µï¼Œå¹¶ä½¿ç”¨ CSS é€‰æ‹©å™¨æå–ç»“æ„åŒ–æ•°æ®ã€‚

## æ ¸å¿ƒèƒ½åŠ›
- ğŸŒ ç½‘é¡µå†…å®¹æŠ“å– (HTML, æ–‡æœ¬, é“¾æ¥)
- ğŸ•µï¸ ç½‘ç«™çˆ¬å– (å¤šé¡µé¢ã€å¤šæ·±åº¦)
- ğŸ” ç½‘é¡µæœç´¢ (Google æœç´¢)
- ğŸ¯ CSS é€‰æ‹©å™¨æ•°æ®æå–
- ğŸ“Š æ•°æ®å¯¼å‡º (JSON, CSV, TXT)
- ğŸ­ åçˆ¬è™«è§„é¿ (éšæœº User-Agent, å»¶è¿Ÿ)

## ä½¿ç”¨åœºæ™¯
- ç½‘é¡µå†…å®¹æŠ“å–
- ç½‘ç«™æ•°æ®çˆ¬å–
- æœç´¢å¼•æ“ç»“æœè·å–
- æ•°æ®æ¸…æ´—å’Œè½¬æ¢
- å¸‚åœºè°ƒç ”å’Œç«äº‰åˆ†æ
- å†…å®¹ç›‘æ§å’Œé€šçŸ¥

## API ç«¯ç‚¹

### æŠ“å–å•ä¸ªé¡µé¢
```bash
clawscrape scrape <url> [options]
```

### çˆ¬å–æ•´ä¸ªç½‘ç«™
```bash
clawscrape crawl <url> [options]
```

### ç½‘é¡µæœç´¢
```bash
clawscrape search <query> [options]
```

### CSS é€‰æ‹©å™¨æå–
```bash
clawscrape extract <html_file> [options]
```

## å®‰è£…ä¾èµ–
```bash
pip install requests beautifulsoup4 lxml
```

## ç¤ºä¾‹ç”¨æ³•

### 1. æŠ“å–å•ä¸ªé¡µé¢
```bash
# æŠ“å–å•ä¸ªé¡µé¢
clawscrape scrape https://example.com

# æŠ“å–å¹¶ä¿å­˜ä¸º JSON
clawscrape scrape https://example.com --output data.json

# æŠ“å–å¹¶ä¿å­˜ä¸º CSV
clawscrape scrape https://example.com --output data.csv --format csv
```

### 2. çˆ¬å–æ•´ä¸ªç½‘ç«™
```bash
# çˆ¬å–ç½‘ç«™ (é»˜è®¤ 10 é¡µï¼Œæ·±åº¦ 3)
clawscrape crawl https://example.com

# çˆ¬å–æ›´å¤šé¡µé¢
clawscrape crawl https://example.com --pages 50 --depth 5

# çˆ¬å–å¹¶ä¿å­˜
clawscrape crawl https://example.com --output site_data.json
```

### 3. ç½‘é¡µæœç´¢
```bash
# æœç´¢ç½‘é¡µ
clawscrape search "OpenClaw AI agent"

# æœç´¢å¹¶ä¿å­˜ç»“æœ
clawscrape search "OpenClaw AI agent" --output results.json
```

### 4. CSS é€‰æ‹©å™¨æå–
```bash
# ä½¿ç”¨ CSS é€‰æ‹©å™¨æå–æ•°æ®
clawscrape extract page.html --selector="title=h1" --selector="content=.main"

# å¤šä¸ªé€‰æ‹©å™¨
clawscrape extract article.html \
  --selector="title=h1" \
  --selector="author=.author" \
  --selector="date=.date" \
  --selector="content=.body"
```

## æ”¯æŒçš„åŠŸèƒ½

### æŠ“å–åŠŸèƒ½
- âœ… HTML å†…å®¹æŠ“å–
- âœ… æ–‡æœ¬æå–
- âœ… é“¾æ¥æå–
- âœ… å›¾ç‰‡æå–
- âœ… è¡¨æ ¼æå–
- âœ… å“åº”çŠ¶æ€ç æ£€æŸ¥
- âœ… é”™è¯¯å¤„ç†å’Œé‡è¯•

### çˆ¬å–åŠŸèƒ½
- âœ… å¤šé¡µé¢çˆ¬å–
- âœ… æ·±åº¦é™åˆ¶
- âœ… é“¾æ¥å‘ç°å’Œè·Ÿéš
- âœ… URL è§„èŒƒåŒ–
- âœ… é‡å¤ URL è¿‡æ»¤

### æœç´¢åŠŸèƒ½
- âœ… Google æœç´¢
- âœ… ç»“æœæå–
- âœ… åˆ†é¡µæ”¯æŒ
- âœ… æœç´¢é™åˆ¶

### æ•°æ®æå–
- âœ… CSS é€‰æ‹©å™¨
- âœ… å±æ€§æå–
- âœ… æ–‡æœ¬æå–
- âœ… å¤šå…ƒç´ æ”¯æŒ

### è¾“å‡ºæ ¼å¼
- âœ… JSON
- âœ… CSV
- âœ… TXT
- âœ… æ–‡ä»¶è‡ªåŠ¨ä¿å­˜

## å‘½ä»¤è¡Œå·¥å…·

### clawscrape å‘½ä»¤
```bash
clawscrape [options] <command> [args]

Commands:
  scrape      æŠ“å–å•ä¸ªé¡µé¢
  crawl       çˆ¬å–æ•´ä¸ªç½‘ç«™
  search      ç½‘é¡µæœç´¢
  extract     CSS é€‰æ‹©å™¨æå–

Options:
  --help      æ˜¾ç¤ºå¸®åŠ©
  --version   æ˜¾ç¤ºç‰ˆæœ¬
  --verbose   è¯¦ç»†è¾“å‡º
  --quiet     å®‰é™æ¨¡å¼
```

## é…ç½®é€‰é¡¹
```json
{
  "headers": {
    "User-Agent": "Mozilla/5.0...",
    "Accept": "text/html..."
  },
  "timeout": 30,
  "delay": 1,
  "max_pages": 100,
  "max_depth": 3,
  "user_agents": [...],
  "proxies": [],
  "output_dir": "~/.clawhub/scraping"
}
```

## Python API
```python
from web_scraper import WebScraper

# åˆå§‹åŒ–
scraper = WebScraper()

# æŠ“å–é¡µé¢
result = scraper.scrape_page('https://example.com')
print(result['title'])
print(result['links'])
print(result['texts'])

# çˆ¬å–ç½‘ç«™
results = scraper.crawl('https://example.com', max_pages=10, max_depth=3)

# æœç´¢
results = scraper.search('OpenClaw AI agent')

# æå–æ•°æ®
selectors = {
    'title': 'h1',
    'content': '.main-content'
}
data = scraper.extract_data(html, selectors)

# ä¿å­˜æ•°æ®
filepath = scraper.save_data([result], 'output.json')
```

## æœ€ä½³å®è·µ
1. always add delay between requests to avoid rate limiting
2. use appropriate User-Agent strings
3. respect robots.txt and website terms
4. handle errors gracefully
5. save data regularly
6. limit crawl depth to avoid excessive requests
7. use CSS selectors for precise data extraction
8. validate extracted data before processing

## æœªæ¥åŠŸèƒ½
- ğŸš€ JavaScript æ¸²æŸ“æ”¯æŒ (Playwright/Selenium)
- ğŸš€ API ç«¯ç‚¹çˆ¬å–
- ğŸš€ åˆ†å¸ƒå¼çˆ¬å–
- ğŸš€ ä»£ç†æ± æ”¯æŒ
- ğŸš€ åçˆ¬è™«ç»•è¿‡ (Cloudflare, Captcha)
- ğŸš€ å®æ—¶æ•°æ®æµ
- ğŸš€ æ•°æ®å¢é‡æ›´æ–°

## è®¸å¯è¯
MIT License

## è´¡çŒ®
æ¬¢è¿æäº¤ Issue å’Œ Pull Request!

## è”ç³»æ–¹å¼
- GitHub: https://github.com/openclaw/web-scraper
- Discord: #clawhub-scraping channel
